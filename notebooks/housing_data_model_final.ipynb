{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katon\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/katon/Documents/JHU/DataVisualization/final_project/data/final_data/\"\n",
    "\n",
    "# independent variables\n",
    "population_df = pd.read_excel(path+\"population.xlsx\").set_index('State')\n",
    "housing_df = pd.read_excel(path+\"housing.xlsx\").set_index('State')\n",
    "\n",
    "# dependent variables\n",
    "homeless_df = pd.read_excel(path+\"homeless.xlsx\").set_index('State')\n",
    "hpi_df = pd.read_excel(path+\"hpi.xlsx\").set_index('State')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def common_indices(dataframes):\n",
    "    common_idx = set(dataframes[0].index)\n",
    "    for df in dataframes[1:]:\n",
    "        common_idx = common_idx.intersection(df.index)\n",
    "    return list(common_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_index = common_indices([homeless_df, housing_df, hpi_df, population_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_list, population_list, homeless_list, hpi_list, state_list, year_list = [], [], [], [], [], []\n",
    "for state in common_index:\n",
    "    for year in list(housing_df.columns):\n",
    "        housing_list.append(housing_df.loc[state, year])\n",
    "        population_list.append(population_df.loc[state, year])\n",
    "        homeless_list.append(homeless_df.loc[state, year])\n",
    "        hpi_list.append(hpi_df.loc[state, year])\n",
    "        state_list.append(state)\n",
    "        year_list.append(year-2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeless_input = pd.DataFrame([housing_list, population_list, hpi_list, state_list, year_list, homeless_list]).T\n",
    "homeless_input.columns = ['housing', 'population', 'hpi', 'state', 'year', 'homeless']\n",
    "\n",
    "hpi_input = pd.DataFrame([housing_list, population_list, state_list, year_list, hpi_list]).T\n",
    "hpi_input.columns = ['housing', 'population', 'state', 'year', 'hpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep hpi\n",
    "hpi_ohe = pd.get_dummies(hpi_input['state'])\n",
    "hpi_input_df = pd.concat([hpi_input[['housing', 'population', 'year', 'hpi']], hpi_ohe], axis=1)\n",
    "hpi_input_df = hpi_input_df.astype(int)\n",
    "hpiX, hpiY = hpi_input_df.drop('hpi', axis=1), hpi_input_df['hpi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep homeless\n",
    "homeless_ohe = pd.get_dummies(homeless_input['state'])\n",
    "homeless_input_df = pd.concat([homeless_input[['housing', 'population', 'hpi', 'year', 'homeless']], homeless_ohe], axis=1)\n",
    "homeless_input_df = homeless_input_df.astype(int)\n",
    "homelessX, homelessY = homeless_input_df.drop('homeless', axis=1), homeless_input_df['homeless']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Start with prediction model for HPI, use HPI prediction as feature for homeless prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katon\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 354413.6048 - val_loss: 361402.3438\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 342800.6544 - val_loss: 339109.4062\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 290086.6967 - val_loss: 251961.1406\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 196412.0901 - val_loss: 75645.9062\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 57850.5742 - val_loss: 41818.0898\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 40670.6739 - val_loss: 26310.5195\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23330.6301 - val_loss: 19445.4883\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15316.7480 - val_loss: 14894.8799\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11772.4378 - val_loss: 11827.0303\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10983.8746 - val_loss: 9424.0352\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9250.1963 - val_loss: 7831.6128\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6867.9276 - val_loss: 6624.6978\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5366.9645 - val_loss: 5716.1279\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5014.9329 - val_loss: 5150.5215\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5884.9129 - val_loss: 4752.8418\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4158.4765 - val_loss: 4596.7583\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3371.3754 - val_loss: 4331.7529\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2873.0764 - val_loss: 4319.0981\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2892.4579 - val_loss: 4038.4746\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3232.9871 - val_loss: 3963.7251\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3435.4631 - val_loss: 3816.7324\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2981.6519 - val_loss: 3848.7847\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3132.4452 - val_loss: 3662.9751\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2901.6382 - val_loss: 3689.7576\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2955.3349 - val_loss: 3494.9182\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2918.6004 - val_loss: 3517.9314\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2716.0524 - val_loss: 3684.1311\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2641.8931 - val_loss: 3534.3442\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2706.6751 - val_loss: 3600.6057\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3157.5816 - val_loss: 3525.0791\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2876.1877 - val_loss: 3433.5613\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2766.9711 - val_loss: 3278.5447\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2419.6703 - val_loss: 3330.8989\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2775.1175 - val_loss: 3291.6265\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2322.3077 - val_loss: 3306.4983\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3269.2065 - val_loss: 3173.9624\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2515.4023 - val_loss: 3188.1025\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2551.0641 - val_loss: 3132.8447\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2734.4275 - val_loss: 3001.3618\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2498.0339 - val_loss: 3021.9810\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2516.2192 - val_loss: 3050.1475\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2862.7046 - val_loss: 3055.1440\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2824.9117 - val_loss: 3088.8665\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1959.9876 - val_loss: 3066.0127\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2824.7267 - val_loss: 3029.0544\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1964.8358 - val_loss: 2909.7476\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2158.3621 - val_loss: 2963.7603\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2293.1789 - val_loss: 2905.0432\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2043.5575 - val_loss: 2958.6794\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2607.6818 - val_loss: 2920.1970\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2390.0170 - val_loss: 3029.1104\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2373.7960 - val_loss: 2882.3516\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2579.7165 - val_loss: 2814.1846\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2270.6424 - val_loss: 2777.7827\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2280.4450 - val_loss: 2783.2449\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2039.6205 - val_loss: 2995.6772\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2028.9571 - val_loss: 2824.4746\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1878.0532 - val_loss: 2710.8960\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2122.8784 - val_loss: 2703.6147\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1944.0934 - val_loss: 2668.4292\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1974.7933 - val_loss: 2676.1108\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2199.8061 - val_loss: 2636.3767\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2537.1862 - val_loss: 2519.4409\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1804.1391 - val_loss: 2623.1016\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1985.9289 - val_loss: 2461.4641\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2039.1037 - val_loss: 2379.7163\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1840.8518 - val_loss: 2520.8162\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1698.9581 - val_loss: 2584.7529\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1769.7004 - val_loss: 2425.9775\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1555.4677 - val_loss: 2412.0728\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1856.5829 - val_loss: 2368.4290\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1858.5201 - val_loss: 2256.5986\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1852.0675 - val_loss: 2164.1492\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1853.0542 - val_loss: 2196.3760\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1506.4291 - val_loss: 2181.7500\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1809.3046 - val_loss: 2130.6660\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1321.6452 - val_loss: 2092.7476\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 1492.4329 - val_loss: 1946.1317\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1505.8091 - val_loss: 1954.4108\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1429.5669 - val_loss: 2035.2355\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1113.3319 - val_loss: 1884.2444\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1384.3994 - val_loss: 1866.8556\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1017.4694 - val_loss: 1795.1647\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1159.0536 - val_loss: 1759.2281\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1152.6598 - val_loss: 1689.8540\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 985.1180 - val_loss: 1674.4677\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 921.7771 - val_loss: 1838.2815\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1276.7839 - val_loss: 1661.2250\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1202.1134 - val_loss: 1471.6608\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1033.0940 - val_loss: 1391.2412\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 727.0179 - val_loss: 1516.0746\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 998.3913 - val_loss: 1279.4716\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 782.1511 - val_loss: 1275.2096\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1168.2715 - val_loss: 1223.0394\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 894.1091 - val_loss: 1249.0009\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 667.5747 - val_loss: 1179.1844\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 562.1813 - val_loss: 1246.4357\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 817.3912 - val_loss: 1141.5852\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 649.7216 - val_loss: 1025.4301\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 482.7628 - val_loss: 1034.0070\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1034.0070\n"
     ]
    }
   ],
   "source": [
    "# HPI\n",
    "hpi_XTrain, hpi_XTest, hpi_yTrain, hpi_yTest = train_test_split(hpiX, hpiY, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data using StandardScaler\n",
    "hpi_scaler = MinMaxScaler()\n",
    "hpi_XTrain_scaled = hpi_scaler.fit_transform(hpi_XTrain)\n",
    "hpi_XTest_scaled = hpi_scaler.transform(hpi_XTest)\n",
    "\n",
    "# Define the model architecture\n",
    "hpi_model = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "hpi_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model\n",
    "hpi_model.fit(hpi_XTrain_scaled, hpi_yTrain, epochs=100, batch_size=32, validation_data=(hpi_XTest_scaled, hpi_yTest))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss = hpi_model.evaluate(hpi_XTest_scaled, hpi_yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 11ms/step - loss: 524955774.1176 - val_loss: 1079089152.0000\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 407597844.7059 - val_loss: 1078606976.0000\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 557931169.8824 - val_loss: 1076314240.0000\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 449669630.1176 - val_loss: 1068506752.0000\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 404941584.0000 - val_loss: 1048039744.0000\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 359817275.2941 - val_loss: 1004405248.0000\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 587573304.4706 - val_loss: 933502848.0000\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 410587574.5882 - val_loss: 851863552.0000\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 240976059.2941 - val_loss: 772035008.0000\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 235416364.0000 - val_loss: 705930304.0000\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 240335365.6471 - val_loss: 662357632.0000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 281660466.8235 - val_loss: 623788672.0000\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 232310537.4118 - val_loss: 585747904.0000\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 227799284.7059 - val_loss: 546850048.0000\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 181541624.4706 - val_loss: 494230784.0000\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 179169926.5882 - val_loss: 445876320.0000\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 142205451.2941 - val_loss: 397923392.0000\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 100205484.7059 - val_loss: 359180800.0000\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 161044263.5294 - val_loss: 296182208.0000\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 96280299.7647 - val_loss: 264425872.0000\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 98585980.9412 - val_loss: 229373648.0000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 62619324.7059 - val_loss: 188814096.0000\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 53263230.7647 - val_loss: 162103952.0000\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 52910919.7647 - val_loss: 138131536.0000\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 45417098.3529 - val_loss: 121584952.0000\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 36400952.1176 - val_loss: 112893608.0000\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31258351.4118 - val_loss: 94303600.0000\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 36535832.3529 - val_loss: 84836584.0000\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25664577.0588 - val_loss: 81751640.0000\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22417535.0000 - val_loss: 69780848.0000\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24165080.5882 - val_loss: 64130448.0000\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18601426.5294 - val_loss: 60732844.0000\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15798221.8824 - val_loss: 54846084.0000\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17447572.0588 - val_loss: 51609812.0000\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22963360.1176 - val_loss: 47336556.0000\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 14053924.4926 - val_loss: 43955448.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19617859.6471 - val_loss: 44569416.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11963870.5882 - val_loss: 40259108.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8722947.3676 - val_loss: 38370072.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12603819.5294 - val_loss: 36697616.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9129853.8382 - val_loss: 35786636.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13393869.4118 - val_loss: 35251320.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9950337.3824 - val_loss: 33932796.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9134106.7647 - val_loss: 32194504.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7019154.9265 - val_loss: 32426228.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9265319.3529 - val_loss: 29178546.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8247716.5588 - val_loss: 27061292.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8948707.4412 - val_loss: 26912222.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10230995.2353 - val_loss: 29092088.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11485548.2941 - val_loss: 27457836.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9834865.8824 - val_loss: 27228046.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9075821.9118 - val_loss: 27450764.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9649192.3456 - val_loss: 30695316.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8659935.7059 - val_loss: 28686764.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10119207.8529 - val_loss: 26048614.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8819776.1176 - val_loss: 27823630.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8385160.8529 - val_loss: 26282156.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7637404.0735 - val_loss: 25675198.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7708889.1912 - val_loss: 27172330.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11025233.8824 - val_loss: 26434318.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6503923.8235 - val_loss: 25921364.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8733996.9338 - val_loss: 31460822.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9224456.0294 - val_loss: 25597622.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9303206.5147 - val_loss: 27662442.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6283195.6691 - val_loss: 26651086.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9115363.5588 - val_loss: 25111076.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7419544.5368 - val_loss: 25456556.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8666913.5882 - val_loss: 28566194.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6338560.5735 - val_loss: 26738368.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6072172.9118 - val_loss: 26681706.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8966020.9118 - val_loss: 23793828.0000\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 11521590.7647 - val_loss: 27944004.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 10807682.0294 - val_loss: 26461298.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8846352.0588 - val_loss: 26434188.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8647992.1176 - val_loss: 26934684.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7184489.2941 - val_loss: 24396652.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7349652.8088 - val_loss: 26632276.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6025423.6912 - val_loss: 25803696.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10243627.8235 - val_loss: 24784630.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8936561.1471 - val_loss: 25052410.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8419280.8088 - val_loss: 25412308.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12188210.9412 - val_loss: 27816220.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10790782.1176 - val_loss: 27279574.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8318644.8382 - val_loss: 26510454.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7164147.1250 - val_loss: 28029550.0000\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9368826.9412 - val_loss: 25188056.0000\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5990568.1415 - val_loss: 26354322.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9616186.4265 - val_loss: 24008782.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7200092.4559 - val_loss: 25322894.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6757572.4559 - val_loss: 25328240.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6821484.8529 - val_loss: 25685290.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10276244.5294 - val_loss: 25738802.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9327457.7647 - val_loss: 27630778.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8584920.7647 - val_loss: 26187790.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8229951.8529 - val_loss: 26521594.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7717742.8824 - val_loss: 28439222.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5540489.5588 - val_loss: 26462350.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9691060.5000 - val_loss: 26169812.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10305543.2537 - val_loss: 25791340.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8885277.6471 - val_loss: 28236592.0000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 28236592.0000\n"
     ]
    }
   ],
   "source": [
    "# Homeless\n",
    "homeless_XTrain, homeless_XTest, homeless_yTrain, homeless_yTest = train_test_split(homelessX, homelessY, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data using StandardScaler\n",
    "homeless_scaler = MinMaxScaler()\n",
    "homeless_XTrain_scaled = homeless_scaler.fit_transform(homeless_XTrain)\n",
    "homeless_XTest_scaled = homeless_scaler.transform(homeless_XTest)\n",
    "\n",
    "# Define the model architecture\n",
    "homeless_model = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "homeless_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model\n",
    "homeless_model.fit(homeless_XTrain_scaled, homeless_yTrain, epochs=100, batch_size=32, validation_data=(homeless_XTest_scaled, homeless_yTest))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss = homeless_model.evaluate(homeless_XTest_scaled, homeless_yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NE', 'AK', 'RI', 'SD', 'MT', 'MD', 'NV', 'ID', 'MA', 'OH', 'CT',\n",
       "       'UT', 'VT', 'AL', 'OR', 'VA', 'GA', 'CO', 'WV', 'NM', 'ND', 'FL',\n",
       "       'CA', 'DC', 'AR', 'WY', 'AZ', 'DE', 'KS', 'NC', 'IN', 'MN', 'MO',\n",
       "       'ME', 'TN', 'NY', 'WI', 'IA', 'IL', 'NH', 'HI', 'MS', 'OK', 'KY',\n",
       "       'LA', 'TX', 'WA', 'NJ', 'SC', 'PA', 'MI'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpi_input.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "The purpose of these models is to make predictions of future HPI and homelessness numbers based on a user inputting the number of houses constructed per year. In order to have this input be done within a Tableau dashboard, the predictions will be premade. This way, the model with not be required each time to make predictions, the dashboard will simply pull from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_construction = housing_df.copy()\n",
    "for year in housing_construction.columns[1:]:\n",
    "    housing_construction[year] = housing_df[year] - housing_df[year-1]\n",
    "mean_units_per_year = np.mean(housing_construction.iloc[:, 1:], axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_scale_hpi(hpi_preds, val_23_100):\n",
    "    diff = val_23_100 - hpi_preds.loc[100, 2023][0][0] \n",
    "    for pct in hpi_preds.index:\n",
    "        for year in hpi_preds.columns:\n",
    "            hpi_preds.loc[pct, year] = hpi_preds.loc[pct, year][0][0] + diff\n",
    "    return hpi_preds\n",
    "\n",
    "def post_scale_homeless(homeless_preds, val_23_100):\n",
    "    diff = val_23_100 - homeless_preds.loc[100, 2023][0][0] \n",
    "    for pct in homeless_preds.index:\n",
    "        for year in homeless_preds.columns:\n",
    "            homeless_preds.loc[pct, year] = homeless_preds.loc[pct, year][0][0] + diff\n",
    "    return homeless_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HPI_dfs = {}\n",
    "HOMELESS_dfs = {}\n",
    "\n",
    "for state in hpi_input.state.unique():\n",
    "\n",
    "    percentages = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "    years = list(range(2023, 2040))\n",
    "\n",
    "    mean_state_units = mean_units_per_year[state]\n",
    "\n",
    "    state_hpi_df = pd.DataFrame(index=percentages, columns=years)\n",
    "    state_homeless_df = pd.DataFrame(index=percentages, columns=years)\n",
    "\n",
    "    for year in state_hpi_df.columns:\n",
    "        for percent in state_hpi_df.index:\n",
    "            housing_val = int(housing_df.loc[state, 2021] + ((year-2021) * ((percent/100) * mean_units_per_year[state]))) # housing var\n",
    "            population_val = population_df.loc[state, year] # population var \n",
    "\n",
    "            # Predict HPI value\n",
    "            hpi_in = pd.DataFrame(columns=hpiX.columns)\n",
    "            hpi_in.loc[0, :] = 0\n",
    "            hpi_in.loc[0, 'housing'] = housing_val\n",
    "            hpi_in.loc[0, 'population'] = population_val\n",
    "            hpi_in.loc[0, 'year'] = year-2000\n",
    "            hpi_in.loc[0, state] = 1\n",
    "            # make prediction\n",
    "            hpi_in = hpi_scaler.transform(hpi_in)\n",
    "            hpi_prediction = hpi_model.predict(hpi_in)\n",
    "            state_hpi_df.loc[percent, year] = hpi_prediction\n",
    "\n",
    "    # Post scale predictions\n",
    "    diff_21_22 = hpi_df.loc[state, 2022] - hpi_df.loc[state, 2021]\n",
    "    hpi_val_23_100 = hpi_df.loc[state, 2022] + diff_21_22\n",
    "    state_hpi_df = post_scale_hpi(state_hpi_df, hpi_val_23_100)\n",
    "\n",
    "\n",
    "    for year in state_hpi_df.columns:\n",
    "        for percent in state_hpi_df.index:\n",
    "            housing_val = int(housing_df.loc[state, 2021] + ((year-2021) * ((percent/100) * mean_units_per_year[state]))) # housing var\n",
    "            population_val = population_df.loc[state, year] # population var \n",
    "            # Predict homeless value\n",
    "            homeless_in = pd.DataFrame(columns=homelessX.columns)\n",
    "            homeless_in.loc[0,:] = 0\n",
    "            homeless_in.loc[0, 'housing'] = housing_val\n",
    "            homeless_in.loc[0, 'population'] = population_val\n",
    "            homeless_in.loc[0, 'hpi'] = state_hpi_df.loc[percent, year]\n",
    "            homeless_in.loc[0, 'year'] = year-2000\n",
    "            homeless_in.loc[0, state] = 1\n",
    "            homeless_in = homeless_scaler.transform(homeless_in)\n",
    "            # make prediction\n",
    "            homeless_prediction = homeless_model.predict(homeless_in)\n",
    "            state_homeless_df.loc[percent, year] = homeless_prediction\n",
    "\n",
    "    # Post scale predictions\n",
    "    diff_21_22 = homeless_df.loc[state, 2022] - homeless_df.loc[state, 2021]\n",
    "    homeless_val_23_100 = homeless_df.loc[state, 2022] + diff_21_22\n",
    "    state_homeless_df = post_scale_homeless(state_homeless_df, homeless_val_23_100)\n",
    "    \n",
    "    # Add to dict\n",
    "    HPI_dfs[state] = state_hpi_df\n",
    "    HOMELESS_dfs[state] = state_homeless_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape into dfs with 2-level header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape HPI\n",
    "hpi_list = []\n",
    "for key, df in HPI_dfs.items():\n",
    "    hpi_list.append(df)\n",
    "hpi_df_final = pd.concat(hpi_list, axis=1, keys=HPI_dfs.keys())\n",
    "\n",
    "# Reshape Homeless\n",
    "homeless_list = []\n",
    "for key, df in HOMELESS_dfs.items():\n",
    "    homeless_list.append(df)\n",
    "homeless_df_final = pd.concat(homeless_list, axis=1, keys=HOMELESS_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">NE</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">MI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "      <th>2025</th>\n",
       "      <th>2026</th>\n",
       "      <th>2027</th>\n",
       "      <th>2028</th>\n",
       "      <th>2029</th>\n",
       "      <th>2030</th>\n",
       "      <th>2031</th>\n",
       "      <th>2032</th>\n",
       "      <th>...</th>\n",
       "      <th>2030</th>\n",
       "      <th>2031</th>\n",
       "      <th>2032</th>\n",
       "      <th>2033</th>\n",
       "      <th>2034</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2037</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>729.009817</td>\n",
       "      <td>782.642263</td>\n",
       "      <td>836.57366</td>\n",
       "      <td>890.602468</td>\n",
       "      <td>944.949697</td>\n",
       "      <td>999.310415</td>\n",
       "      <td>1053.734426</td>\n",
       "      <td>1108.194265</td>\n",
       "      <td>1162.680105</td>\n",
       "      <td>1217.17156</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.957556</td>\n",
       "      <td>1150.332556</td>\n",
       "      <td>1204.76554</td>\n",
       "      <td>1259.225012</td>\n",
       "      <td>1313.688391</td>\n",
       "      <td>1368.168738</td>\n",
       "      <td>1422.655432</td>\n",
       "      <td>1477.160315</td>\n",
       "      <td>1531.665564</td>\n",
       "      <td>1586.170691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>729.009878</td>\n",
       "      <td>782.642507</td>\n",
       "      <td>836.57427</td>\n",
       "      <td>890.603262</td>\n",
       "      <td>944.949636</td>\n",
       "      <td>999.310476</td>\n",
       "      <td>1053.733328</td>\n",
       "      <td>1108.193044</td>\n",
       "      <td>1162.678884</td>\n",
       "      <td>1217.169851</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.956091</td>\n",
       "      <td>1150.331091</td>\n",
       "      <td>1204.76261</td>\n",
       "      <td>1259.221838</td>\n",
       "      <td>1313.68656</td>\n",
       "      <td>1368.167029</td>\n",
       "      <td>1422.653845</td>\n",
       "      <td>1477.159216</td>\n",
       "      <td>1531.663977</td>\n",
       "      <td>1586.168982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>729.009817</td>\n",
       "      <td>782.642751</td>\n",
       "      <td>836.574819</td>\n",
       "      <td>890.604116</td>\n",
       "      <td>944.949331</td>\n",
       "      <td>999.310415</td>\n",
       "      <td>1053.732351</td>\n",
       "      <td>1108.191946</td>\n",
       "      <td>1162.677175</td>\n",
       "      <td>1217.168142</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.954749</td>\n",
       "      <td>1150.329504</td>\n",
       "      <td>1204.759558</td>\n",
       "      <td>1259.21842</td>\n",
       "      <td>1313.684363</td>\n",
       "      <td>1368.164709</td>\n",
       "      <td>1422.652747</td>\n",
       "      <td>1477.157629</td>\n",
       "      <td>1531.662634</td>\n",
       "      <td>1586.167639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>729.01</td>\n",
       "      <td>782.64269</td>\n",
       "      <td>836.57543</td>\n",
       "      <td>890.60491</td>\n",
       "      <td>944.949148</td>\n",
       "      <td>999.310232</td>\n",
       "      <td>1053.73113</td>\n",
       "      <td>1108.190603</td>\n",
       "      <td>1162.675588</td>\n",
       "      <td>1217.166311</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.95304</td>\n",
       "      <td>1150.327917</td>\n",
       "      <td>1204.75614</td>\n",
       "      <td>1259.215002</td>\n",
       "      <td>1313.682654</td>\n",
       "      <td>1368.162878</td>\n",
       "      <td>1422.651648</td>\n",
       "      <td>1477.156531</td>\n",
       "      <td>1531.661169</td>\n",
       "      <td>1586.166174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>729.01</td>\n",
       "      <td>782.642812</td>\n",
       "      <td>836.576101</td>\n",
       "      <td>890.605703</td>\n",
       "      <td>944.948965</td>\n",
       "      <td>999.310171</td>\n",
       "      <td>1053.730276</td>\n",
       "      <td>1108.189749</td>\n",
       "      <td>1162.674124</td>\n",
       "      <td>1217.164724</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.951697</td>\n",
       "      <td>1150.326331</td>\n",
       "      <td>1204.75321</td>\n",
       "      <td>1259.211584</td>\n",
       "      <td>1313.680701</td>\n",
       "      <td>1368.160559</td>\n",
       "      <td>1422.650305</td>\n",
       "      <td>1477.155066</td>\n",
       "      <td>1531.660071</td>\n",
       "      <td>1586.164587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>729.010061</td>\n",
       "      <td>782.642996</td>\n",
       "      <td>836.576772</td>\n",
       "      <td>890.606375</td>\n",
       "      <td>944.948599</td>\n",
       "      <td>999.31011</td>\n",
       "      <td>1053.729299</td>\n",
       "      <td>1108.188528</td>\n",
       "      <td>1162.67217</td>\n",
       "      <td>1217.162771</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.950476</td>\n",
       "      <td>1150.324622</td>\n",
       "      <td>1204.750159</td>\n",
       "      <td>1259.208044</td>\n",
       "      <td>1313.678992</td>\n",
       "      <td>1368.158484</td>\n",
       "      <td>1422.648962</td>\n",
       "      <td>1477.153601</td>\n",
       "      <td>1531.65824</td>\n",
       "      <td>1586.163123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>729.010122</td>\n",
       "      <td>782.643118</td>\n",
       "      <td>836.577444</td>\n",
       "      <td>890.607107</td>\n",
       "      <td>944.948538</td>\n",
       "      <td>999.310049</td>\n",
       "      <td>1053.728079</td>\n",
       "      <td>1108.187307</td>\n",
       "      <td>1162.670706</td>\n",
       "      <td>1217.16094</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.948645</td>\n",
       "      <td>1150.322913</td>\n",
       "      <td>1204.746741</td>\n",
       "      <td>1259.204626</td>\n",
       "      <td>1313.67655</td>\n",
       "      <td>1368.156531</td>\n",
       "      <td>1422.647864</td>\n",
       "      <td>1477.152258</td>\n",
       "      <td>1531.656653</td>\n",
       "      <td>1586.16178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>729.010183</td>\n",
       "      <td>782.64324</td>\n",
       "      <td>836.577993</td>\n",
       "      <td>890.608145</td>\n",
       "      <td>944.948293</td>\n",
       "      <td>999.309927</td>\n",
       "      <td>1053.727102</td>\n",
       "      <td>1108.185964</td>\n",
       "      <td>1162.668997</td>\n",
       "      <td>1217.159109</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.947424</td>\n",
       "      <td>1150.321448</td>\n",
       "      <td>1204.743933</td>\n",
       "      <td>1259.201331</td>\n",
       "      <td>1313.674963</td>\n",
       "      <td>1368.154578</td>\n",
       "      <td>1422.646399</td>\n",
       "      <td>1477.15116</td>\n",
       "      <td>1531.655432</td>\n",
       "      <td>1586.159949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>729.010305</td>\n",
       "      <td>782.64324</td>\n",
       "      <td>836.578848</td>\n",
       "      <td>890.608938</td>\n",
       "      <td>944.948049</td>\n",
       "      <td>999.309805</td>\n",
       "      <td>1053.726125</td>\n",
       "      <td>1108.184866</td>\n",
       "      <td>1162.667532</td>\n",
       "      <td>1217.157644</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.945959</td>\n",
       "      <td>1150.319617</td>\n",
       "      <td>1204.741003</td>\n",
       "      <td>1259.198035</td>\n",
       "      <td>1313.67301</td>\n",
       "      <td>1368.152502</td>\n",
       "      <td>1422.644934</td>\n",
       "      <td>1477.149817</td>\n",
       "      <td>1531.654089</td>\n",
       "      <td>1586.158728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>729.010427</td>\n",
       "      <td>782.643484</td>\n",
       "      <td>836.579458</td>\n",
       "      <td>890.609731</td>\n",
       "      <td>944.947805</td>\n",
       "      <td>999.309683</td>\n",
       "      <td>1053.725149</td>\n",
       "      <td>1108.183767</td>\n",
       "      <td>1162.665945</td>\n",
       "      <td>1217.155691</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.944495</td>\n",
       "      <td>1150.318274</td>\n",
       "      <td>1204.737585</td>\n",
       "      <td>1259.194617</td>\n",
       "      <td>1313.670813</td>\n",
       "      <td>1368.150183</td>\n",
       "      <td>1422.64408</td>\n",
       "      <td>1477.14823</td>\n",
       "      <td>1531.652625</td>\n",
       "      <td>1586.156897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>729.010427</td>\n",
       "      <td>782.643606</td>\n",
       "      <td>836.579885</td>\n",
       "      <td>890.610464</td>\n",
       "      <td>944.947561</td>\n",
       "      <td>999.309683</td>\n",
       "      <td>1053.724172</td>\n",
       "      <td>1108.182546</td>\n",
       "      <td>1162.664114</td>\n",
       "      <td>1217.153982</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.942908</td>\n",
       "      <td>1150.316565</td>\n",
       "      <td>1204.734656</td>\n",
       "      <td>1259.191199</td>\n",
       "      <td>1313.669226</td>\n",
       "      <td>1368.147986</td>\n",
       "      <td>1422.642249</td>\n",
       "      <td>1477.146643</td>\n",
       "      <td>1531.65116</td>\n",
       "      <td>1586.155432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>729.010549</td>\n",
       "      <td>782.643728</td>\n",
       "      <td>836.580557</td>\n",
       "      <td>890.611135</td>\n",
       "      <td>944.947378</td>\n",
       "      <td>999.3095</td>\n",
       "      <td>1053.723196</td>\n",
       "      <td>1108.18157</td>\n",
       "      <td>1162.662649</td>\n",
       "      <td>1217.152151</td>\n",
       "      <td>...</td>\n",
       "      <td>1095.941443</td>\n",
       "      <td>1150.314978</td>\n",
       "      <td>1204.731604</td>\n",
       "      <td>1259.188391</td>\n",
       "      <td>1313.667151</td>\n",
       "      <td>1368.145911</td>\n",
       "      <td>1422.641272</td>\n",
       "      <td>1477.145667</td>\n",
       "      <td>1531.649817</td>\n",
       "      <td>1586.154089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 867 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             NE                                                              \\\n",
       "           2023        2024        2025        2026        2027        2028   \n",
       "25   729.009817  782.642263   836.57366  890.602468  944.949697  999.310415   \n",
       "50   729.009878  782.642507   836.57427  890.603262  944.949636  999.310476   \n",
       "75   729.009817  782.642751  836.574819  890.604116  944.949331  999.310415   \n",
       "100      729.01   782.64269   836.57543   890.60491  944.949148  999.310232   \n",
       "125      729.01  782.642812  836.576101  890.605703  944.948965  999.310171   \n",
       "150  729.010061  782.642996  836.576772  890.606375  944.948599   999.31011   \n",
       "175  729.010122  782.643118  836.577444  890.607107  944.948538  999.310049   \n",
       "200  729.010183   782.64324  836.577993  890.608145  944.948293  999.309927   \n",
       "225  729.010305   782.64324  836.578848  890.608938  944.948049  999.309805   \n",
       "250  729.010427  782.643484  836.579458  890.609731  944.947805  999.309683   \n",
       "275  729.010427  782.643606  836.579885  890.610464  944.947561  999.309683   \n",
       "300  729.010549  782.643728  836.580557  890.611135  944.947378    999.3095   \n",
       "\n",
       "                                                         ...           MI  \\\n",
       "            2029         2030         2031         2032  ...         2030   \n",
       "25   1053.734426  1108.194265  1162.680105   1217.17156  ...  1095.957556   \n",
       "50   1053.733328  1108.193044  1162.678884  1217.169851  ...  1095.956091   \n",
       "75   1053.732351  1108.191946  1162.677175  1217.168142  ...  1095.954749   \n",
       "100   1053.73113  1108.190603  1162.675588  1217.166311  ...   1095.95304   \n",
       "125  1053.730276  1108.189749  1162.674124  1217.164724  ...  1095.951697   \n",
       "150  1053.729299  1108.188528   1162.67217  1217.162771  ...  1095.950476   \n",
       "175  1053.728079  1108.187307  1162.670706   1217.16094  ...  1095.948645   \n",
       "200  1053.727102  1108.185964  1162.668997  1217.159109  ...  1095.947424   \n",
       "225  1053.726125  1108.184866  1162.667532  1217.157644  ...  1095.945959   \n",
       "250  1053.725149  1108.183767  1162.665945  1217.155691  ...  1095.944495   \n",
       "275  1053.724172  1108.182546  1162.664114  1217.153982  ...  1095.942908   \n",
       "300  1053.723196   1108.18157  1162.662649  1217.152151  ...  1095.941443   \n",
       "\n",
       "                                                                      \\\n",
       "            2031         2032         2033         2034         2035   \n",
       "25   1150.332556   1204.76554  1259.225012  1313.688391  1368.168738   \n",
       "50   1150.331091   1204.76261  1259.221838   1313.68656  1368.167029   \n",
       "75   1150.329504  1204.759558   1259.21842  1313.684363  1368.164709   \n",
       "100  1150.327917   1204.75614  1259.215002  1313.682654  1368.162878   \n",
       "125  1150.326331   1204.75321  1259.211584  1313.680701  1368.160559   \n",
       "150  1150.324622  1204.750159  1259.208044  1313.678992  1368.158484   \n",
       "175  1150.322913  1204.746741  1259.204626   1313.67655  1368.156531   \n",
       "200  1150.321448  1204.743933  1259.201331  1313.674963  1368.154578   \n",
       "225  1150.319617  1204.741003  1259.198035   1313.67301  1368.152502   \n",
       "250  1150.318274  1204.737585  1259.194617  1313.670813  1368.150183   \n",
       "275  1150.316565  1204.734656  1259.191199  1313.669226  1368.147986   \n",
       "300  1150.314978  1204.731604  1259.188391  1313.667151  1368.145911   \n",
       "\n",
       "                                                         \n",
       "            2036         2037         2038         2039  \n",
       "25   1422.655432  1477.160315  1531.665564  1586.170691  \n",
       "50   1422.653845  1477.159216  1531.663977  1586.168982  \n",
       "75   1422.652747  1477.157629  1531.662634  1586.167639  \n",
       "100  1422.651648  1477.156531  1531.661169  1586.166174  \n",
       "125  1422.650305  1477.155066  1531.660071  1586.164587  \n",
       "150  1422.648962  1477.153601   1531.65824  1586.163123  \n",
       "175  1422.647864  1477.152258  1531.656653   1586.16178  \n",
       "200  1422.646399   1477.15116  1531.655432  1586.159949  \n",
       "225  1422.644934  1477.149817  1531.654089  1586.158728  \n",
       "250   1422.64408   1477.14823  1531.652625  1586.156897  \n",
       "275  1422.642249  1477.146643   1531.65116  1586.155432  \n",
       "300  1422.641272  1477.145667  1531.649817  1586.154089  \n",
       "\n",
       "[12 rows x 867 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpi_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-1a5996b94719>:2: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.7' currently installed).\n",
      "  hpi_df_final.to_excel(path+'final_hpi_projections.xlsx')\n",
      "<ipython-input-20-1a5996b94719>:3: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.7' currently installed).\n",
      "  homeless_df_final.to_excel(path+'final_homeless_projections.xlsx')\n"
     ]
    }
   ],
   "source": [
    "# Write to Excel\n",
    "hpi_df_final.to_excel(path+'final_hpi_projections.xlsx')\n",
    "homeless_df_final.to_excel(path+'final_homeless_projections.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
