{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katon\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/katon/Documents/JHU/DataVisualization/final_project/data/final_data/\"\n",
    "\n",
    "# independent variables\n",
    "population_df = pd.read_excel(path+\"population.xlsx\").set_index('State')\n",
    "housing_df = pd.read_excel(path+\"housing.xlsx\").set_index('State')\n",
    "\n",
    "# dependent variables\n",
    "homeless_df = pd.read_excel(path+\"homeless.xlsx\").set_index('State')\n",
    "hpi_df = pd.read_excel(path+\"hpi.xlsx\").set_index('State')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def common_indices(dataframes):\n",
    "    common_idx = set(dataframes[0].index)\n",
    "    for df in dataframes[1:]:\n",
    "        common_idx = common_idx.intersection(df.index)\n",
    "    return list(common_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_index = common_indices([homeless_df, housing_df, hpi_df, population_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_list, population_list, homeless_list, hpi_list, state_list, year_list = [], [], [], [], [], []\n",
    "for state in common_index:\n",
    "    for year in list(housing_df.columns):\n",
    "        housing_list.append(housing_df.loc[state, year])\n",
    "        population_list.append(population_df.loc[state, year])\n",
    "        homeless_list.append(homeless_df.loc[state, year])\n",
    "        hpi_list.append(hpi_df.loc[state, year])\n",
    "        state_list.append(state)\n",
    "        year_list.append(year-2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeless_input = pd.DataFrame([housing_list, population_list, hpi_list, state_list, year_list, homeless_list]).T\n",
    "homeless_input.columns = ['housing', 'population', 'hpi', 'state', 'year', 'homeless']\n",
    "\n",
    "hpi_input = pd.DataFrame([housing_list, population_list, state_list, year_list, hpi_list]).T\n",
    "hpi_input.columns = ['housing', 'population', 'state', 'year', 'hpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep hpi\n",
    "hpi_ohe = pd.get_dummies(hpi_input['state'])\n",
    "hpi_input_df = pd.concat([hpi_input[['housing', 'population', 'year', 'hpi']], hpi_ohe], axis=1)\n",
    "hpi_input_df = hpi_input_df.astype(int)\n",
    "hpiX, hpiY = hpi_input_df.drop('hpi', axis=1), hpi_input_df['hpi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep homeless\n",
    "homeless_ohe = pd.get_dummies(homeless_input['state'])\n",
    "homeless_input_df = pd.concat([homeless_input[['housing', 'population', 'hpi', 'year', 'homeless']], homeless_ohe], axis=1)\n",
    "homeless_input_df = homeless_input_df.astype(int)\n",
    "homelessX, homelessY = homeless_input_df.drop('homeless', axis=1), homeless_input_df['homeless']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Start with prediction model for HPI, use HPI prediction as feature for homeless prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katon\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 354413.6048 - val_loss: 361402.3438\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 342800.6544 - val_loss: 339109.4062\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 290086.6967 - val_loss: 251961.1406\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 196412.0901 - val_loss: 75645.9062\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 57850.5742 - val_loss: 41818.0898\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 40670.6739 - val_loss: 26310.5195\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23330.6301 - val_loss: 19445.4883\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15316.7480 - val_loss: 14894.8799\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11772.4378 - val_loss: 11827.0303\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10983.8746 - val_loss: 9424.0352\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9250.1963 - val_loss: 7831.6128\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6867.9276 - val_loss: 6624.6978\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5366.9645 - val_loss: 5716.1279\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5014.9329 - val_loss: 5150.5215\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5884.9129 - val_loss: 4752.8418\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4158.4765 - val_loss: 4596.7583\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3371.3754 - val_loss: 4331.7529\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2873.0764 - val_loss: 4319.0981\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2892.4579 - val_loss: 4038.4746\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3232.9871 - val_loss: 3963.7251\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3435.4631 - val_loss: 3816.7324\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2981.6519 - val_loss: 3848.7847\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3132.4452 - val_loss: 3662.9751\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2901.6382 - val_loss: 3689.7576\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2955.3349 - val_loss: 3494.9182\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2918.6004 - val_loss: 3517.9314\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2716.0524 - val_loss: 3684.1311\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2641.8931 - val_loss: 3534.3442\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2706.6751 - val_loss: 3600.6057\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3157.5816 - val_loss: 3525.0791\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2876.1877 - val_loss: 3433.5613\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2766.9711 - val_loss: 3278.5447\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2419.6703 - val_loss: 3330.8989\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2775.1175 - val_loss: 3291.6265\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2322.3077 - val_loss: 3306.4983\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3269.2065 - val_loss: 3173.9624\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2515.4023 - val_loss: 3188.1025\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2551.0641 - val_loss: 3132.8447\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2734.4275 - val_loss: 3001.3618\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2498.0339 - val_loss: 3021.9810\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2516.2192 - val_loss: 3050.1475\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2862.7046 - val_loss: 3055.1440\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2824.9117 - val_loss: 3088.8665\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1959.9876 - val_loss: 3066.0127\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2824.7267 - val_loss: 3029.0544\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1964.8358 - val_loss: 2909.7476\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2158.3621 - val_loss: 2963.7603\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2293.1789 - val_loss: 2905.0432\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2043.5575 - val_loss: 2958.6794\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2607.6818 - val_loss: 2920.1970\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2390.0170 - val_loss: 3029.1104\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2373.7960 - val_loss: 2882.3516\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2579.7165 - val_loss: 2814.1846\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2270.6424 - val_loss: 2777.7827\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2280.4450 - val_loss: 2783.2449\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2039.6205 - val_loss: 2995.6772\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2028.9571 - val_loss: 2824.4746\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1878.0532 - val_loss: 2710.8960\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2122.8784 - val_loss: 2703.6147\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1944.0934 - val_loss: 2668.4292\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1974.7933 - val_loss: 2676.1108\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2199.8061 - val_loss: 2636.3767\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2537.1862 - val_loss: 2519.4409\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1804.1391 - val_loss: 2623.1016\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1985.9289 - val_loss: 2461.4641\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2039.1037 - val_loss: 2379.7163\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1840.8518 - val_loss: 2520.8162\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1698.9581 - val_loss: 2584.7529\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1769.7004 - val_loss: 2425.9775\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1555.4677 - val_loss: 2412.0728\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1856.5829 - val_loss: 2368.4290\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1858.5201 - val_loss: 2256.5986\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1852.0675 - val_loss: 2164.1492\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1853.0542 - val_loss: 2196.3760\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1506.4291 - val_loss: 2181.7500\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1809.3046 - val_loss: 2130.6660\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1321.6452 - val_loss: 2092.7476\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 1492.4329 - val_loss: 1946.1317\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1505.8091 - val_loss: 1954.4108\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1429.5669 - val_loss: 2035.2355\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1113.3319 - val_loss: 1884.2444\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1384.3994 - val_loss: 1866.8556\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1017.4694 - val_loss: 1795.1647\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1159.0536 - val_loss: 1759.2281\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1152.6598 - val_loss: 1689.8540\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 985.1180 - val_loss: 1674.4677\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 921.7771 - val_loss: 1838.2815\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1276.7839 - val_loss: 1661.2250\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1202.1134 - val_loss: 1471.6608\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1033.0940 - val_loss: 1391.2412\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 727.0179 - val_loss: 1516.0746\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 998.3913 - val_loss: 1279.4716\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 782.1511 - val_loss: 1275.2096\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1168.2715 - val_loss: 1223.0394\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 894.1091 - val_loss: 1249.0009\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 667.5747 - val_loss: 1179.1844\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 562.1813 - val_loss: 1246.4357\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 817.3912 - val_loss: 1141.5852\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 649.7216 - val_loss: 1025.4301\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 482.7628 - val_loss: 1034.0070\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1034.0070\n"
     ]
    }
   ],
   "source": [
    "# HPI\n",
    "hpi_XTrain, hpi_XTest, hpi_yTrain, hpi_yTest = train_test_split(hpiX, hpiY, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data using StandardScaler\n",
    "hpi_scaler = MinMaxScaler()\n",
    "hpi_XTrain_scaled = hpi_scaler.fit_transform(hpi_XTrain)\n",
    "hpi_XTest_scaled = hpi_scaler.transform(hpi_XTest)\n",
    "\n",
    "# Define the model architecture\n",
    "hpi_model = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "hpi_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model\n",
    "hpi_model.fit(hpi_XTrain_scaled, hpi_yTrain, epochs=100, batch_size=32, validation_data=(hpi_XTest_scaled, hpi_yTest))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss = hpi_model.evaluate(hpi_XTest_scaled, hpi_yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8ee1dd10a315>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Scale the input data using StandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhomeless_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mhomeless_XTrain_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhomeless_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhomeless_XTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mhomeless_XTest_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhomeless_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhomeless_XTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Homeless\n",
    "homeless_XTrain, homeless_XTest, homeless_yTrain, homeless_yTest = train_test_split(homelessX, homelessY, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data using StandardScaler\n",
    "homeless_scaler = StandardScaler()\n",
    "homeless_XTrain_scaled = homeless_scaler.fit_transform(homeless_XTrain)\n",
    "homeless_XTest_scaled = homeless_scaler.transform(homeless_XTest)\n",
    "\n",
    "# Define the model architecture\n",
    "homeless_model = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    #layers.Dropout(0.1),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "homeless_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model\n",
    "homeless_model.fit(homeless_XTrain_scaled, homeless_yTrain, epochs=100, batch_size=32, validation_data=(homeless_XTest_scaled, homeless_yTest))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss = homeless_model.evaluate(homeless_XTest_scaled, homeless_yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_input.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "The purpose of these models is to make predictions of future HPI and homelessness numbers based on a user inputting the number of houses constructed per year. In order to have this input be done within a Tableau dashboard, the predictions will be premade. This way, the model with not be required each time to make predictions, the dashboard will simply pull from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_construction = housing_df.copy()\n",
    "for year in housing_construction.columns[1:]:\n",
    "    housing_construction[year] = housing_df[year] - housing_df[year-1]\n",
    "mean_units_per_year = np.mean(housing_construction.iloc[:, 1:], axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_scale_hpi(hpi_preds, val_23_100):\n",
    "    diff = val_23_100 - hpi_preds.loc[100, 2023][0][0] \n",
    "    for pct in hpi_preds.index:\n",
    "        for year in hpi_preds.columns:\n",
    "            hpi_preds.loc[pct, year] = hpi_preds.loc[pct, year][0][0] + diff\n",
    "    return hpi_preds\n",
    "\n",
    "def post_scale_homeless(homeless_preds, val_23_100):\n",
    "    diff = val_23_100 - homeless_preds.loc[100, 2023][0][0] \n",
    "    for pct in homeless_preds.index:\n",
    "        for year in homeless_preds.columns:\n",
    "            homeless_preds.loc[pct, year] = homeless_preds.loc[pct, year][0][0] + diff\n",
    "    return homeless_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HPI_dfs = {}\n",
    "HOMELESS_dfs = {}\n",
    "\n",
    "for state in hpi_input.state.unique():\n",
    "\n",
    "    percentages = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "    years = list(range(2023, 2040))\n",
    "\n",
    "    mean_state_units = mean_units_per_year[state]\n",
    "\n",
    "    state_hpi_df = pd.DataFrame(index=percentages, columns=years)\n",
    "    state_homeless_df = pd.DataFrame(index=percentages, columns=years)\n",
    "\n",
    "    for year in state_hpi_df.columns:\n",
    "        for percent in state_hpi_df.index:\n",
    "            housing_val = int(housing_df.loc[state, 2021] + ((year-2021) * ((percent/100) * mean_units_per_year[state]))) # housing var\n",
    "            population_val = population_df.loc[state, year] # population var \n",
    "\n",
    "            # Predict HPI value\n",
    "            hpi_in = pd.DataFrame(columns=hpiX.columns)\n",
    "            hpi_in.loc[0, :] = 0\n",
    "            hpi_in.loc[0, 'housing'] = housing_val\n",
    "            hpi_in.loc[0, 'population'] = population_val\n",
    "            hpi_in.loc[0, 'year'] = year-2000\n",
    "            hpi_in.loc[0, state] = 1\n",
    "            # make prediction\n",
    "            hpi_in = hpi_scaler.transform(hpi_in)\n",
    "            hpi_prediction = hpi_model.predict(hpi_in)\n",
    "            state_hpi_df.loc[percent, year] = hpi_prediction\n",
    "\n",
    "    # Post scale predictions\n",
    "    diff_21_22 = hpi_df.loc[state, 2022] - hpi_df.loc[state, 2021]\n",
    "    hpi_val_23_100 = hpi_df.loc[state, 2022] + diff_21_22\n",
    "    state_hpi_df = post_scale_hpi(state_hpi_df, hpi_val_23_100)\n",
    "\n",
    "\n",
    "    for year in state_hpi_df.columns:\n",
    "        for percent in state_hpi_df.index:\n",
    "            housing_val = int(housing_df.loc[state, 2021] + ((year-2021) * ((percent/100) * mean_units_per_year[state]))) # housing var\n",
    "            population_val = population_df.loc[state, year] # population var \n",
    "            # Predict homeless value\n",
    "            homeless_in = pd.DataFrame(columns=homelessX.columns)\n",
    "            homeless_in.loc[0,:] = 0\n",
    "            homeless_in.loc[0, 'housing'] = housing_val\n",
    "            homeless_in.loc[0, 'population'] = population_val\n",
    "            homeless_in.loc[0, 'hpi'] = state_hpi_df.loc[percent, year]\n",
    "            homeless_in.loc[0, 'year'] = year-2000\n",
    "            homeless_in.loc[0, state] = 1\n",
    "            homeless_in = homeless_scaler.transform(homeless_in)\n",
    "            # make prediction\n",
    "            homeless_prediction = homeless_model.predict(homeless_in)\n",
    "            state_homeless_df.loc[percent, year] = homeless_prediction\n",
    "\n",
    "    # Post scale predictions\n",
    "    diff_21_22 = homeless_df.loc[state, 2022] - homeless_df.loc[state, 2021]\n",
    "    homeless_val_23_100 = homeless_df.loc[state, 2022] + diff_21_22\n",
    "    state_homeless_df = post_scale_homeless(state_homeless_df, homeless_val_23_100)\n",
    "    \n",
    "    # Add to dict\n",
    "    HPI_dfs[state] = state_hpi_df\n",
    "    HOMELESS_dfs[state] = state_homeless_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape into dfs with 2-level header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape HPI\n",
    "hpi_list = []\n",
    "for key, df in HPI_dfs.items():\n",
    "    hpi_list.append(df)\n",
    "hpi_df_final = pd.concat(hpi_list, axis=1, keys=HPI_dfs.keys())\n",
    "\n",
    "# Reshape Homeless\n",
    "homeless_list = []\n",
    "for key, df in HOMELESS_dfs.items():\n",
    "    homeless_list.append(df)\n",
    "homeless_df_final = pd.concat(homeless_list, axis=1, keys=HOMELESS_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Excel\n",
    "hpi_df_final.to_excel(path+'final_hpi_projections.xlsx')\n",
    "homeless_df_final.to_excel(path+'final_homeless_projections.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
